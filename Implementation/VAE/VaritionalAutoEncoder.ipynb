{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras import backend as K\r\n",
    "from tensorflow.keras.datasets import mnist\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "print(f\"tensorflow {tf.__version__}\")\r\n",
    "print(f\"Numpy {np.__version__}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensorflow 2.5.0\n",
      "Numpy 1.19.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![VAE](VAE.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Sampling(tf.keras.layers.Layer):\r\n",
    "\tdef __init__(self):\r\n",
    "\t\tsuper(Sampling, self).__init__()\r\n",
    "\tdef call(self, inputs):\r\n",
    "\t\tz_mean,z_log_var = inputs\r\n",
    "\t\tprint(z_mean)\r\n",
    "\t\tbatch_data = tf.shape(z_mean)[0]\r\n",
    "\t\tdim_data = tf.shape(z_mean)[1]\r\n",
    "\t\tepsilon = K.random_normal(shape= (batch_data, dim_data), mean=0., stddev=1.0)\r\n",
    "\t\treturn z_mean + tf.exp(z_log_var * 0.5) * epsilon\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\r\n",
    "BATCH_SIZE = 64,\r\n",
    "ORIGINAL_DIM = 28 * 28\r\n",
    "\r\n",
    "LATENT_DIM = 2 # tHE cOMPRESSED dIMENSION/ Bottleneck\r\n",
    "INTERMEDIATE_DIM = 256 # The units For Decoder and Encoder\r\n",
    "EPOCHS = 5 #  Total Epochs\r\n",
    "epsilon_std = 1.0 # For generating The Gaussian \r\n",
    "\r\n",
    "### E N C O D E R ####\r\n",
    "x = Input(shape=(28,28,1), name=\"Input_Images\") # input Layers , just Pass the Original Dims , Vector with 784 elements\r\n",
    "h = Dense(INTERMEDIATE_DIM,activation=tf.nn.relu, name=\"Encoding\")(x) # This Layers is For encoding the Input\r\n",
    "h = tf.keras.layers.Flatten()(h) # This Layers is For Flattening the Input\r\n",
    "z_mean = Dense(LATENT_DIM, name=\"Mean\")(h) # This layer Function is For Getting the Mean of the Gaussian that we're going to create\r\n",
    "z_log_var = Dense(LATENT_DIM, name=\"Log_Variance\")(h)  # This Layer is for getting the variance of the Gaussian but the LOG , its same as the variance but more stable\r\n",
    "z = Sampling()([z_mean, z_log_var]) # This is the Sampling Function , this will be used for getting the latent vector\r\n",
    "encoder = Model(x, [z_mean, z_log_var,z], name=\"Encoder\")  # Creating the models for the Encoder\r\n",
    "print(encoder.summary())\r\n",
    "### E N C O D E R ####\r\n",
    "#### D E C O D E R ####\r\n",
    "input_decoder = Input(shape=(LATENT_DIM,), name='decoder_input') # This Is Input for decoder\r\n",
    "decoder_h = Dense(INTERMEDIATE_DIM, activation=tf.nn.relu,name=\"decoder_h\")(input_decoder) # This is The layers for decoder\r\n",
    "input_decoded = Dense(ORIGINAL_DIM,activation=tf.nn.sigmoid,name=\"Flattened_decoded\")(decoder_h) # This layers is for The output < Reconstruted input > \r\n",
    "reshaped = tf.reshape(input_decoded, (-1,28,28,1)) # Reshape the input , -1 For Batch\r\n",
    "decoder = Model(input_decoder,reshaped,name=\"decoder\") # Creating the Decoder Models\r\n",
    "print(decoder.summary())\r\n",
    "## Sampling / Reparameterization trick\r\n",
    "\r\n",
    "####  Creating Variational AutoEncoder ####\r\n",
    "#print(create_decoder().summary())\r\n",
    "#### D E C O D E R ####\r\n",
    "class VAE(tf.keras.Model):\r\n",
    "\tdef __init__ (self,encoder,decoder,**kwargs):\r\n",
    "\t\tsuper(VAE,self).__init__()\r\n",
    "\t\tself.encoder = encoder\r\n",
    "\t\tself.decoder = decoder\r\n",
    "\r\n",
    "\tdef train_step(self,data):\r\n",
    "\t\tif isinstance(data, tuple):\r\n",
    "\t\t\tdata = data[0]\r\n",
    "\t\twith tf.GradientTape() as tape:\r\n",
    "\t\t\tz_mean,z_log_var,z = self.encoder(data)\r\n",
    "\t\t\t\r\n",
    "\t\t\treconstruction  = self.decoder(z)\r\n",
    "\t\t\treconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(data,reconstruction))\r\n",
    "\t\t\treconstruction_loss *= ORIGINAL_DIM\r\n",
    "\t\t\t\r\n",
    "\t\t\tkl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\r\n",
    "\t\t\tkl_loss = tf.reduce_sum(kl_loss,axis=None)\r\n",
    "\t\t\tkl_loss *= -0.5\r\n",
    "\r\n",
    "\t\t\ttotal_loss = reconstruction_loss + kl_loss\r\n",
    "\t\tgrads = tape.gradient(total_loss, self.trainable_variables)\r\n",
    "\r\n",
    "\t\tself.optimizer.apply_gradients(zip(grads, self.trainable_variables))\r\n",
    "\t\treturn {\r\n",
    "\t\t\t\"loss\" : total_loss,\r\n",
    "\t\t\t\"reconstruction_loss\" : reconstruction_loss,\r\n",
    "\t\t\t\"kl_loss\" : kl_loss\r\n",
    "\t\t}\r\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.fashion_mnist.load_data()\r\n",
    "fmnist_images = np.concatenate([x_train, x_test], axis = 0)\r\n",
    "# expand dimension to add  a color map dimension\r\n",
    "fmnist_images = np.expand_dims(fmnist_images, -1).astype(\"float32\") / 255\r\n",
    "  \r\n",
    "# compile and train the model\r\n",
    "vae = VAE(encoder, decoder)\r\n",
    "vae.compile(optimizer ='rmsprop')\r\n",
    "vae.fit(fmnist_images, epochs = 100, batch_size = 64)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(None, 2), dtype=float32)\n",
      "Model: \"Encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_Images (InputLayer)       [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoding (Dense)                (None, 28, 28, 256)  512         Input_Images[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 200704)       0           Encoding[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Mean (Dense)                    (None, 2)            401410      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Log_Variance (Dense)            (None, 2)            401410      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sampling_11 (Sampling)          (None, 2)            0           Mean[0][0]                       \n",
      "                                                                 Log_Variance[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 803,332\n",
      "Trainable params: 803,332\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "decoder_h (Dense)            (None, 256)               768       \n",
      "_________________________________________________________________\n",
      "Flattened_decoded (Dense)    (None, 784)               201488    \n",
      "_________________________________________________________________\n",
      "tf.reshape_11 (TFOpLambda)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 202,256\n",
      "Trainable params: 202,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "Tensor(\"Encoder/Mean/BiasAdd:0\", shape=(None, 2), dtype=float32)\n",
      "Tensor(\"Encoder/Mean/BiasAdd:0\", shape=(None, 2), dtype=float32)\n",
      "1094/1094 [==============================] - 164s 148ms/step - loss: 3791.0207 - reconstruction_loss: 384.8151 - kl_loss: 3406.2056\n",
      "Epoch 2/100\n",
      " 458/1094 [===========>..................] - ETA: 1:34 - loss: 382.3809 - reconstruction_loss: 367.1988 - kl_loss: 15.1821"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
    "\r\n",
    "x_train = x_train.astype('float32') / 255.\r\n",
    "x_test = x_test.astype('float32') / 255.\r\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\r\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vae.fit(x_train, x_train,\r\n",
    "        shuffle=True,\r\n",
    "        epochs=EPOCHS,)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "('Error when checking model target: expected no data, but got:', array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-40a09ebedfd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m vae.fit(x_train, x_train,\n\u001b[0;32m      2\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         epochs=EPOCHS,)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2467\u001b[0m           \u001b[0mshapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2468\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2469\u001b[1;33m           exception_prefix='target')\n\u001b[0m\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m       \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DataScience\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    494\u001b[0m       raise ValueError(\n\u001b[0;32m    495\u001b[0m           \u001b[1;34m'Error when checking model '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m           'expected no data, but got:', data)\n\u001b[0m\u001b[0;32m    497\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('Error when checking model target: expected no data, but got:', array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('DataScience': conda)"
  },
  "interpreter": {
   "hash": "81e8d6ec8b128683c3ce30ee758ecb536232cfad698fab650870c6728bfc659d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}