{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "print(f\"tf version {tf.__version__}\")\r\n",
    "print(f\"Numpy version {np.__version__}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "BATCH_SIZE = 64 \r\n",
    "EPOCHS = 5\r\n",
    "class Encoder(tf.keras.layers.Layer):\r\n",
    "\t\"\"\"\r\n",
    "\tEncoder Layer , To Learned the features of input data and  extract the important vfeatures\r\n",
    "\r\n",
    "\tArgs:\r\n",
    "\t    tf (Layer): Model Subclassing \r\n",
    "\t\"\"\"\r\n",
    "\tdef __init__(self, Intermediate_dims):\r\n",
    "\t\tsuper(Encoder, self).__init__()\r\n",
    "\t\t\r\n",
    "\t\tself.hidden_layer = tf.keras.layers.Dense(\r\n",
    "\t\t\tunits = Intermediate_dims,\r\n",
    "\t\t\tactivation = tf.nn.relu,\r\n",
    "\t\t\tkernel_initializer = tf.keras.initializers.he_normal(),\r\n",
    "\t\t\tname = 'hidden_layer'\r\n",
    "\t\t)\r\n",
    "\t\tself.output_layer = tf.keras.layers.Dense(\r\n",
    "\t\t\tunits = Intermediate_dims,\r\n",
    "\t\t\tactivation = tf.nn.sigmoid\r\n",
    "\t\t)\r\n",
    "\t\r\n",
    "\tdef call(self, input_features):\r\n",
    "\t\tactivation = self.hidden_layer(input_features)\r\n",
    "\t\treturn self.output_layer(activation)\r\n",
    "\r\n",
    "\r\n",
    "class Decoder(tf.keras.layers.Layer):\r\n",
    "\tdef __init__(self, intermediate_dims, original_dim):\r\n",
    "\t\tsuper(Decoder,self).__init__()\r\n",
    "\t\tself.hidden_layer = tf.keras.layers.Dense(\r\n",
    "\t\t\tunits = intermediate_dims,\r\n",
    "\t\t\tactivation = tf.nn.relu,\r\n",
    "\t\t\tkernel_initializer = tf.keras.initializers.he_normal(),\r\n",
    "\t\t)\r\n",
    "\t\tself.output_layer = tf.keras.layers.Dense(\r\n",
    "\t\t\tunits = original_dim, ## this Units Is Responsible For the output, Later we Will reshape the output\r\n",
    "\t\t\tactivation = tf.nn.sigmoid\r\n",
    "\t\t)\r\n",
    "\r\n",
    "\tdef call(self,input_features):\r\n",
    "\t\tactivation = self.hidden_layer(input_features)\r\n",
    "\t\treturn self.output_layer(activation)\r\n",
    "\t\r\n",
    "\r\n",
    "class AutoEncoder(tf.keras.layers.Layer):\r\n",
    "\tdef __init__(self, intermediate_dims, original_dim):\r\n",
    "\t\tsuper(AutoEncoder, self).__init__()\r\n",
    "\t\tself.encoder = Encoder(intermediate_dims)\r\n",
    "\t\tself.decoder = Decoder(intermediate_dims, original_dim)\r\n",
    "\r\n",
    "\tdef call(self, input_features):\r\n",
    "\t\tz = self.encoder(input_features)\r\n",
    "\t\treconstructed_features = self.decoder(z)\r\n",
    "\t\treturn reconstructed_features\r\n",
    "\r\n",
    "def loss(ModelData , originalData):\r\n",
    "\treconstruction_error = tf.reduce_mean(tf.square(tf.subtract(ModelData(originalData) , originalData)))\r\n",
    "\treturn reconstruction_error\r\n",
    "\r\n",
    "def train(loss, modelData, optimizer ,originalData ):\r\n",
    "\twith tf.GradientTape() as tape:\r\n",
    "\t\tgenerated_loss = loss(modelData,originalData)\r\n",
    "\t\tloss_value = tape.gradient(generated_loss, modelData.trainable_variables)\r\n",
    "\t\tgradient_variables = zip(loss_value, modelData.trainable_variables)\r\n",
    "\t\toptimizer.apply_gradients(gradient_variables)\r\n",
    "\r\n",
    "autoencoder = AutoEncoder(intermediate_dims=64, original_dim=784)\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n",
    "\r\n",
    "(training_features, _ ), (test_features, _) = tf.keras.datasets.mnist.load_data()\r\n",
    "training_features = training_features / 255.0\r\n",
    "test_features = test_features / 255.0\r\n",
    "training_features = training_features.reshape(training_features.shape[0],\r\n",
    "\t\t\t\t\t\ttraining_features.shape[1] * training_features.shape[2])\r\n",
    "training_features = training_features.astype(\"float32\")\r\n",
    "\r\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(training_features)\r\n",
    "training_dataset = training_dataset.batch(BATCH_SIZE).shuffle(training_features.shape[0]).prefetch(BATCH_SIZE * 4) \r\n",
    "\r\n",
    "writer = tf.summary.create_file_writer(\"tmp\")\r\n",
    "\r\n",
    "with writer.as_default():\r\n",
    "\twith tf.summary.record_if(True):\r\n",
    "\t\tfor epoch in range(EPOCHS):\r\n",
    "\t\t\tprint(epoch)\r\n",
    "\t\t\tfor step, batch_features in enumerate(training_dataset):\r\n",
    "\t\t\t\ttrain(loss,autoencoder, optimizer,batch_features)\r\n",
    "\t\t\t\tloss_values = loss(autoencoder, batch_features)\r\n",
    "\t\t\t\toriginal = tf.reshape(batch_features, (batch_features.shape[0], 28, 28 ,1))\r\n",
    "\t\t\t\ttheProposedInput = autoencoder(batch_features)\r\n",
    "\t\t\t\treconstructed = tf.reshape(theProposedInput, (batch_features.shape[0], 28, 28, 1))\r\n",
    "\t\t\t\ttf.summary.scalar(\"Loss\", loss_values, step=step)\r\n",
    "\t\t\t\ttf.summary.image(\"Original\", original, step=step)\r\n",
    "\t\t\t\ttf.summary.image(\"reconstructed\",reconstructed,max_outputs=10,step=step)\r\n",
    "\r\n",
    "\t\t\t\t"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "%tensorboard --logdir tmp/"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('DataScience': conda)"
  },
  "interpreter": {
   "hash": "81e8d6ec8b128683c3ce30ee758ecb536232cfad698fab650870c6728bfc659d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}